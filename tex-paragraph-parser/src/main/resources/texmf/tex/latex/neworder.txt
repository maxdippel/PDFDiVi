New and Improved Submission Format and Policy

What is the new policy?
Why the change?
What is the new format?
How does this affect the put procedure?
How does this affect the get procedure?

---------------------------------------------------------------------

What is the new policy?

Starting January 1, 1996, submitted papers will be required to be processable
by our automatic TeX'ing script before being accepted onto the archives.
This includes everything sent by the `(f)put' and `(f)replace' commands.
(Please note `TeX' is a general term that includes all the various flavors,
including, but not limited to, LaTeX, AMSTeX, and AMSLaTeX).



Why the change?

The change in policy is designed to make the archives as functional and
useful as possible. Auto-TeX'ing has been on-line at the archives since June,
1995. Our script can process about 92% of older TeX submissions and over 95%
of new submissions. The script most often fails because of trivial errors by
the submitter. Yet many authors do not seem to care enough to fix errors in
their papers, leaving readers and the archive administrators the burden
to fix them. The goal is to shift the burden on the author where it belongs,
and thereby ensure that all papers are processable.

Then why not just accept PostScript? As explained in our on-line help,
PostScript is not well-suited for archival purposes. With the TeX source,
papers can be made available in a number of different formats.
Soon to come on-line:

PostScript with custom bitmapped font resolutions (300, 400, and 600 dpi)
PostScript with Type 1 fonts (not embedded)
Portable Document Format (PDF)
DVI

All TeX is processed using HyperTeX so that the formats
listed above are optimized for on-screen reading (with the proper viewers).
Furthermore, TeX source is easily parsed for references to other papers
on the archives (e.g. hep-th/9511053). We translate these
to the appropriate URL's so that the various hyper viewers can bring up the
abstract of the reference in a World Wide Web browser. (All authors should be
supplying archive references in their references list since this is most
useful to readers.) Our auto-TeX'ing script is quite sophisticated and can
handle all flavors of TeX (e.g. LaTeX, AMSTeX, AMSLaTeX) as well as figure
insertions using macros like \psfig and \epsfbox.

Finally, we will probably begin to reject PostScript, if it is produced 
from TeX, in the near future. PostScript should only be submitted
when TeX source doesn't exist.


What is the new format?

The new format is simple. If your submission consists of a single TeX file,
then you just submit the TeX file. If you are submitting a single PostScript
file or PDF file (don't submit these if they are produced from TeX),
then you should gzip the file and uuencode that. If your submisssion consists
of multiple files (e.g. TeX file, style files, PostScript figures, etc.), then
all of the files should be tar'ed, gzip'ed, and uuencoded and put as one
package. There will be no more `add' or `figures' e-mail commands. (Note
uuencoding is not needed if the paper is put using ftp (in binary mode) via
the `fput' command [or soon using the WorldWideWeb upload] .)

The new format will simplify the storage and transmission formats.
It is required by the new policy since all files will be necessary at the
outset for processing.  The other benefit is that submissions will no
longer be stored as compressed tar files containing uuencoded, compressed
or gzipped, tar files, ..., an artifactual format obsoleted by improved
availability of platform-independent utilities.

For more information on where to find the necessary utilities and
how to carry out this packaging, see our uufiles FAQ and 
utilities.txt.



How does this affect the put procedure?

The initial `put' should now be more streamlined, with neither
figures command nor splitting files into several groups for submission.
Also, our empirically set size limits will no longer be a cryptic combination
of factors that depend on whether you use put or fput, or whether you use the
figure and add commands. Instead, there will now be a more uniform size limit
that will be applied to the final storage format (a tar file compressed with
gzip -9) instead of to the uuencoded source sent by submitters. We expect this
limit to be set somewhere between 500kB and 750kB. Submitters, especially
those putting anything over 300kB, should read sizes.txt for guidelines
on creating space-efficient submissions.

People who will experience the most trouble will be those that have mailers
that limit the size of their outgoing mail (limits as low as 100kB are still
around). In these cases, the submitter should use the fput and
freplace commands (also to be improved dramatically).


Currently submitters have been able to ignore the edifying e-mail
from the archive with the only consequence being a nuisance notation
in the event of problems with the submission. Under the new system,
problem-free submissions will require no further action, as before.
If a submission fails to process correctly, however, the submitter will
receive a log file showing the script output (together with the files as
received here to inspect for any e-mail induced corruption at transmission
end) and it will be the submitter's responsibility to fix all errors before
attempting resubmission. Rejected submitters will benefit from reading the
e-mail responses from the archive.


How does this affect getting papers?

WorldWideWeb users will find that new papers will be
available in a variety of PostScript styles. Those still using the `get'
command to retrieve source via e-mail will now see a more uniform packaging
that will simplify unpacking. Figures will no longer be sent separately by
default; instead multipart submissions will be e-mailed as uuencoded gzipped
tar files. In addition, single TeX files will now be sent out gzip'ed and
uuencoded (so that reform to wrap lines longer than 80 characters is no
longer necessary). A new command, `uget', will be introduced to unpack the
multiple file packages and send out the pieces separately for those few
still intimidated by the above format (despite the existence of unpacking
utilities on all platforms, see utilities.txt).
Users with limits on incoming mail will also still be able to use the cget
command to get the source in 100kB pieces of the uuencoded, gzip'ed, tar
file that will have to be concatenated together before unpacking. FTP users
will also notice a simpler packaging.



